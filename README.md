# **IMDb Sentiment Analysis with RoBERTa**  

## ğŸ“– **DescripciÃ³n**  

Este proyecto tiene como objetivo demostrar las capacidades de **fine-tuning** del modelo **RoBERTa (Robustly optimized BERT approach)** en el anÃ¡lisis de sentimientos de reseÃ±as de pelÃ­culas de **IMDb**.  

El objetivo es **clasificar las reseÃ±as como positivas o negativas**, aprovechando la potencia de **Transfer Learning** en modelos de lenguaje preentrenados.  

<p align="center">
  <img src="Fine-Tuning RoBERTa for Sentiment Analysis on IMDb Reviews.jpg" alt="Sentiment Analysis with RoBERTa">
</p>  

## ğŸ¯ **Â¿Por quÃ© RoBERTa?**  

Escogimos **RoBERTa** porque ofrece mejoras significativas sobre el modelo original de BERT:  
âœ… Preentrenado en un conjunto de datos mucho mÃ¡s grande.  
âœ… Optimizaciones en el proceso de entrenamiento para mejorar el rendimiento.  
âœ… Accesibilidad gratuita sin necesidad de API Keys.  

---

## ğŸš€ **Flujo de Trabajo**  

### 1ï¸âƒ£ **Carga y Preprocesamiento de Datos**  
ğŸ“Œ **IMDb Dataset** cargado desde `datasets` de Hugging Face.  
ğŸ“Œ **TokenizaciÃ³n** con `RoBERTaTokenizer`, truncando y rellenando (`padding`) para entradas uniformes.  

### 2ï¸âƒ£ **Fine-Tuning del Modelo**  
ğŸ“Œ **RoBERTaForSequenceClassification** ajustado sobre el conjunto de datos de IMDb.  
ğŸ“Œ OptimizaciÃ³n de hiperparÃ¡metros (`batch_size`, `learning_rate`, `epochs`).  
ğŸ“Œ Ajuste de pesos para mejorar rendimiento en anÃ¡lisis de sentimientos.  

### 3ï¸âƒ£ **EvaluaciÃ³n del Modelo**  
ğŸ“Œ CÃ¡lculo de mÃ©tricas clave:  
   - **F1-score**  
   - **PrecisiÃ³n (`Precision`)**  
   - **Recall**  
   - **Loss de validaciÃ³n**  

---

## ğŸ“‚ **Estructura del Proyecto**  

```
â”œâ”€â”€ preprocess.py   # Preprocesamiento del dataset IMDb
â”œâ”€â”€ train.py        # Entrenamiento del modelo RoBERTa
â”œâ”€â”€ evaluate.py     # EvaluaciÃ³n del modelo
â”œâ”€â”€ results/        # Carpeta donde se guardan los modelos entrenados
â””â”€â”€ README.md       # DocumentaciÃ³n del proyecto
```

---

## ğŸ”¥ **Retos y Adaptaciones**  

ğŸ”¹ **Limitaciones de Recursos:**  
  - **Fine-tuning** de modelos grandes como RoBERTa requiere alta capacidad de cÃ³mputo.  
  - Se optimizaron los **batch sizes** y la carga de datos para evitar saturaciÃ³n.  

ğŸ”¹ **Manejo del Desbalanceo de Clases:**  
  - Aunque IMDb estÃ¡ relativamente balanceado, fue necesario ajustar el entrenamiento para evitar **overfitting**.  

ğŸ”¹ **Longitud de los Textos:**  
  - Las reseÃ±as largas requerÃ­an **tokenizaciÃ³n eficiente** para evitar la pÃ©rdida de informaciÃ³n importante.  

---

## ğŸ“Š **Resultados**  

Obtuvimos un rendimiento sobresaliente en el anÃ¡lisis de sentimientos:  

| **MÃ©trica**           | **Valor**    |
|-----------------------|-------------|
| **Loss de ValidaciÃ³n** | 0.2524      |
| **PrecisiÃ³n (`Accuracy`)** | 95.39% |
| **F1-score**         | 0.9541      |
| **PrecisiÃ³n (`Precision`)** | 0.9497      |
| **Recall**           | 0.9586      |

Estos resultados indican que **RoBERTa ofrece un balance Ã³ptimo entre precisiÃ³n y generalizaciÃ³n**, siendo altamente confiable para el anÃ¡lisis de sentimientos en reseÃ±as de pelÃ­culas.  

---

## ğŸ **ConclusiÃ³n**  

Este proyecto demuestra el **poder del aprendizaje por transferencia (Transfer Learning)** y el **fine-tuning** para la clasificaciÃ³n de sentimientos.  

âœ… **Utilizando RoBERTa**, logramos **alta precisiÃ³n y mÃ©tricas equilibradas** en la clasificaciÃ³n de reseÃ±as de IMDb.  
âœ… **OptimizaciÃ³n de entrenamiento y procesamiento** permitiÃ³ lograr un modelo eficiente con buen rendimiento.  

ğŸ’¡ **Posibles mejoras:**  
- ImplementaciÃ³n de **GridSearchCV** para afinar hiperparÃ¡metros.  
- Uso de **DistilRoBERTa** para reducir el tamaÃ±o del modelo sin perder rendimiento.  
- AplicaciÃ³n de **Data Augmentation** en reseÃ±as de baja representatividad.  

---

## ğŸ›  **Requisitos y ConfiguraciÃ³n**  

AsegÃºrate de instalar los siguientes paquetes antes de ejecutar el cÃ³digo:  
```bash
pip install transformers datasets torch sklearn numpy
```

---

## ğŸ’¬ **Agradecimientos**  

Este proyecto fue desarrollado utilizando:  
ğŸ”¹ **Hugging Face Transformers**  
ğŸ”¹ **Datasets**  
ğŸ”¹ **PyTorch**  

ğŸ“¢ **Si te resultÃ³ Ãºtil, dale un â­ al repositorio y sÃ­gueme en GitHub!** ğŸš€  

---

## ğŸ”— **Contacto**  

ğŸ“Œ **[Web](https://pdroruiz.com/)**  
ğŸ“Œ **[GitHub](https://github.com/pdro-ruiz)**  
ğŸ“Œ **[Kaggle](https://www.kaggle.com/pdroruiz)**  
ğŸ“Œ **[LinkedIn](https://www.linkedin.com/in/)**  

Â¡Gracias por leer! **Nos vemos en el siguiente proyecto!** ğŸ‘‹  

---

Espero que este README sea Ãºtil y atractivo para compartir en plataformas como GitHub o Kaggle. ğŸš€
